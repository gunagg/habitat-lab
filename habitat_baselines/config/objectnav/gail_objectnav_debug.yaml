# Note:  This is an example config, see habitat_baselines/config/pointnav/overfitting_pointnav.yaml
# for better hyperparameters for actual trainingrgb

BASE_TASK_CONFIG_PATH: "configs/tasks/objectnav_mp3d_gail_debug.yaml"
TRAINER_NAME: "gail"
ENV_NAME: NavRLEnv
SIMULATOR_GPU_ID: 0
TORCH_GPU_ID: 0
VIDEO_OPTION: ["disk"]
TENSORBOARD_DIR: "tb/objectnav/gail/overfitting/rgb/v15_full/"
VIDEO_DIR: "video_dir/objectnav/gail/overfitting/rgb/v15_full/"
# To evaluate on all episodes, set this to -1
TEST_EPISODE_COUNT: -1
EVAL_CKPT_PATH_DIR: "checkpoints/objectnav/gail/overfitting/rgb/v15_full/ckpt.5.pth"
SHOW_TOP_DOWN_MAP: False
NUM_PROCESSES: 1
CHECKPOINT_FOLDER: "checkpoints/objectnav/gail/overfitting/rgb/v15_full/"
OUTPUT_LOG_DIR: data/objectnav/logs
LOG_INTERVAL: 10
LOG_METRICS: True
NUM_CHECKPOINTS: -1
CHECKPOINT_INTERVAL: 100
SENSORS: ['RGB_SENSOR', 'DEPTH_SENSOR']
DATASET_PATH: "/srv/flash1/rramrakhya6/habitat-lab/data/datasets/objectnav_mp3d_v2/{split}/{scene_split}.db"
RESULTS_DIR: "data/objectnav/results/gail/overfitting/rgb/{split}/{type}"
EVAL_RESUTLS_DIR: "data/objectnav/results/"
EVAL_SAVE_RESULTS: True
EVAL_SAVE_RESULTS_INTERVAL: 50
EVAL:
  SPLIT: "single_episode"
  sample: False
  semantic_metrics: False
  evaluation_meta_file: "tb/objectnav/gail/overfitting/rgb/v15_full/evaluation_meta_35k_3.json"
NUM_UPDATES: 5000

RL:
  SUCCESS_REWARD: 2.5
  SLACK_REWARD: -1e-3

  PPO:
    # overfitting params
    clip_param: 0.2
    overfitting_epoch: 4 
    num_mini_batch: 1
    value_loss_coef: 0.5
    entropy_coef: 0.01
    lr: 2.5e-4
    eps: 1e-5
    max_grad_norm: 0.2
    num_steps: 64
    use_gae: True
    gamma: 0.99
    tau: 0.95
    use_linear_clip_decay: False
    use_linear_lr_decay: False
    reward_window_size: 50

    use_normalized_advantage: False

    hidden_size: 512

    # Use double buffered sampling, typically helps
    # when environment time is similar or large than
    # policy inference time during rollout generation
    use_double_buffered_sampler: False

IL:
  USE_IW: True
  distrib_backend: GLOO
  GAIL:
    lr: 1.0e-6
    eps: 1.0e-5
    clip_param: 0.2
    num_mini_batch: 1
    max_grad_norm: 0.2
    num_steps: 64
    use_linear_clip_decay: False
    use_linear_lr_decay: True
    reward_window_size: 50
    sync_frac: 0.6

    hidden_size: 512
    embed_actions: False
    force_blind_discriminator: True

    use_normalized_advantage: False
    reward_type: "gail"

    DISCRIMINATOR:
      name: "ObjectNavDiscriminator"

MODEL:
  ablate_depth: False
  ablate_rgb: False
  num_recurrent_layers: 2
  rnn_type: "GRU"
  backbone: "resnet18"
  resnet_baseplanes: 32
  normalize_visual_inputs: False
  force_blind_policy: False
  embed_sge: True
  embed_goal_seg: False
  rgb: False
  NO_VISION: False
  USE_SEMANTICS: False
  USE_PRED_SEMANTICS: False
  SWITCH_TO_PRED_SEMANTICS_UPDATE: 0
  SEMANTIC_ENCODER:
    # rednet_ckpt: "data/rednet-models/rednet_semmap_mp3d_40_v2_vince.pth"
    rednet_ckpt: "data/rednet-models/rednet_semmap_mp3d_tuned.pth"
    cnn_type: "ResnetSemSegEncoder"
    output_size: 256
    backbone: "resnet18"
    train_encoder: True
    embedding_size: 4
    is_thda: True
    num_classes: 29
  RGB_ENCODER:
    cnn_type: "ResnetRGBEncoder"
    output_size: 256
    backbone: "resnet18"
    train_encoder: True
  DEPTH_ENCODER:
    cnn_type: "VlnResnetDepthEncoder"
    output_size: 128
    backbone: "resnet50"
    trainable: False
    ddoverfitting_checkpoint: "data/ddoverfitting-models/gibson-2plus-resnet50.pth"
  STATE_ENCODER:
    hidden_size: 512
    rnn_type: "GRU"
    num_recurrent_layers: 1
  SEQ2SEQ:
    use_prev_action: True
  PROGRESS_MONITOR:
    use: False